Lecture 1: Naive Bayes Classifiers:
	Assumption that each feature is independent 
	Highly efficient learning and prediction
	But generalization performance may be worse than sophisticated learning methods
	Good for high dimension datasets

Bernoulli: 
	binary geatures, word presence and absence
	Multinomical: discrete features
	Gaussian: continuous/ real-valued features
	Well-suited for textual data

Gaussian: Assumes gaussian distribution represents all data

"Check out partial fit for Bayes classifiers in sklearn"

Used for High Dimentional Data

Pros: 
- Easy to understand
- Simple, efficient parameter estimation
- Works well with high-dimensional data
- Often useful as a baseline comparison against more sophisticated methods

Cons: 
- Assumption that features are conditionally independent given the class is not realistics
- As a result, other classifier types often have better generalization performance
- Their confidence estimates for predictions are not very accurate

Lecture 2: Random Forests

Creating learning models known as ensembles: aggregating different learning models

Average out individual mistakes and reduce overfitting

Random Forests are an example
- An ensemble of trees, not just one tree
- Widely used, very good results on many problems
sklearn.ensemble model

Pros: 
- Widely used, excellent prediction performance on many problems
- doesn't require careful normalization of features or extensive parameter tuning
- like decision trees, handles a mixture of feature types
- easily parallelized across multiple CPUs

Cons: 
- resulting models are oftent difficult to interpret 
- like decision trees, random forests may not be a good choice for a very high dimensional task compared to fast accurate linear models

key parameters: 
n_estimates: 
max_features:
max_Depth: 
n_jobs: 

Choose a fixed setting for the random_state parameter if you need reproducible results


Lecture 3: Gradient Boosted Decision Trees

They build a series of trees where each subsequent tree tries to correct the mistakes of the previous trees

Lots of shallow trees (weak learners)
Learning rate: high - more complex trees

Reduce overfitting: reduce learning rate and reduce max depth


Pros: 
- often best off the shelo accuracy on many problems
- using model for prediction requires only modest memory and is fast
- deosn't require careful normalization of features to perform well
- like decision trees, handles a mixture of feature types

Cons: 
- like random forests, the models are often difficult for humans to interpret
- requires careful tuning of the learning rate and other parameters
- training can require significant computation 
- like decision trees, not recommended for text classification and other problems with very high dimensional sparse features, for accuracy and computational cost reasons

Key parameters:
n_estimators - sets number of trees
learning_rate - controls emphasis on fixing errors from previous iteration
n_estimators: 
max_depth: 3-5 for most applications


Lecture 4: Neural Networks

Hidden layer: non linear sum of the input features into this layer

Allows to learn more complex function. A lot more weights and coefficients. 

Activation Functions: 
RELU:  maps any negative input value to zero
Hyperbolic Tangent Function: large values aer mapped to 1, and negative values to negative one

Pros: 
- They form the basis of state-of-the-art models and can be formed into advanced architectures that effectively capture complex features given enough data and computation

Cons: 
- Larger, more complex models require significant training time, data, and customization
- Careful preprocessing of the data is needed 
- A good chioce when the features are of similar types but less so when features of very different types 

Important parameters: 
hidden_layer_sizes: set of number of hidden layes and number of hidden units per layer
alpha: controls weight on the regularization penalty that shrinks weights to zero
activation: controls teh nonlinear function used for the activation functino including "relu", "logistic", "tanh"

Deep Learning: 

- Includes an automatic feature learning phase with a supervised learning pahse

- The feature extraction phase uses a hierarchy of multiple feature extractino layers
- Starting from primitive low, level features in the initial layer,each feature layer's output provides the input features to the next higher feature layer
- ALl features are used in teh final supervised learning model


Data Leakage: 
- When the data you're using to train contains information about what you're trying to predict
- Intorducing information about the target during training that would not legitimately be available during the actual use 
Ex: including the label to be predicted as a feature, including test adata with trianing data
- if you model perforamnce is too good to be true, it is probably due to "giveaway" features

Detecting Data Leakage: 
Before building the model: 
- exploratory data analysis to find surprises in the data
- are there features very highly correlated with the target value?
After building the model: 
- look for suprising feature behavior in the fitted model
- are there features with very high weights, or high information gain?
- simple rule-based models like decision trees can help with features like account numbers, patient ids
- is overall model performance surprisingly good compared to known results on teh same dataset, or for similar problems on similar datasets?
Limited real-world deployment of the trained model
- potentially expensive in terms of development time, but more realistic
- is the trained model generalizing well to new datA?

Minimizing Data Leakage: 
Perforamce data preparation withing each cross validation fold separaetly
With time series data, use a timestamp cutoff
Before any work with a new dataset, split off a final test validation dataset

Unsupervised Learning: 

Transformation: 
- processes taht extract or compute information
Clustering: 
- find groups in the data
- assign every point in the dataset to one of the grap hs

Dimensionality reduction and manifold learning

- Finds an approximate version of your dataset using fewer features
- USed for exploring visualization a dataset to understand grouping or relationships
- Often visualized using a 2-D sctterplot
- Also used for compression, finding features for supervised learning

Two-dimensional dataset to a 1-D approximation

PCA - take your cloud of original data points and finds a rotation of it, so the dimensions are statistically uncorrelated. PCA then typically drops all but the most informative initial dimensions that capture most of the variation int he original dataset

Highest variance - first principal

Manifold learning algorithms are very good at finding low dimensional structure in high dimensional data and are very useful for visualizations.   

t-SNE: a powerful manifold learning method that finds a 2d projection preserving information about neighbors

Clustering: 

Findind a way to divide a dataset into groups ("clusters")

Data points within the same cluster should be 'close' or 'similar' in some way

Clustering algorithms output a cluster membership index for each data point: 
- hard clustering: each data point belongs to exactly one cluster
- soft clustering: each data point is assigned a weight, score, or probability of menbership for each cluster. 

k-means clustering

Limitations: 
- works well for simple clusters that are same size, well-separated, globular shapes
- does not do well with irregular, complex clusters
- variants of k-means like k-mediods can work with categorical features

